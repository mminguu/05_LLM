{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bce331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력한 데이터를 토큰화 및 시퀀스 변화 -> padding(고정길이화) -> 임베딩(단어 -> 벡터화)\n",
    "# 1D Convolution + poling 반복! 반복하는 이유는? 특징을 잡아야하기 때문에.\n",
    "# 반복을 한 다음 Flatten 을 사용하여 Dense(은닉층)을 거쳐 출력(Soft Max , 이진분류)을 하게 됨\n",
    "# 그것을 토대로 학습(Adam + binary_crossentropy)을 하고 손실함수를 이용하여 검증하고 테스트하고 평가를 한 후 시각화를 한다.\n",
    "\n",
    "# 원-핫을 하면 단어 간의 관계를 전혀 알 수 없다.\n",
    "# 학습되는 것 처럼 보이겠지만 학습이 되지 않기 때문에 방향성이 있는 벡터화를 해야한다.\n",
    "# cosine 등을 이용하여 단어간의 유사성을 확인할 수 있음.\n",
    "# 차원이 아무리 많아도 1~3차원을 만들어 규칙을 보고 예측함.\n",
    "\n",
    "# tensorflow를 사용할 시 python 3.11 버전으로 하는게 안전함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e1287",
   "metadata": {},
   "source": [
    "### tensorflow를 사용할 시 python 3.11 버전으로 하는게 안전함.\n",
    "\n",
    "#### 터미널에서 가상환경 만드는 방법\n",
    "\n",
    "conda env list 입력.\n",
    "\n",
    "conda create -n p311 python=3.11 가상환경 만듬. (p311 이 부분은 가상환경 이름.)\n",
    "\n",
    "y/n -> y 입력\n",
    "\n",
    "#### conda의 기본채널에 없는 것들이 있기때문에 그럴때는 pip로 install 하거나 채널을 찾아야함.\n",
    "-------------------------------------------\n",
    "conda env list conda 가상환경들 다 나옴\n",
    "\n",
    "conda activate final  final의 가상환경을 선택.\n",
    "\n",
    "conda list conda에 있는 list들이 다 나옴\n",
    "\n",
    "conda install tensorflow tensorflow 설치.\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "conda remove -n final --all final 디렉토리를 삭제한다는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a7179",
   "metadata": {},
   "source": [
    "- 말뭉치 로딩 (nltk) 데이터 로딩\n",
    "- 토큰화 (잘라야지) 빈고 기반 인덱싱 / 잘게 쪼갠다음 텍스트를 숫자로 변환\n",
    "- 시퀀스 패딩 고정 길이 배치 구성\n",
    "- 임베딩 단어를 밀집형태의 벡터로 표현 및 학습\n",
    "- 임베딩 발전\n",
    "    - 한계 : 작은 데이터에서는 일반화 부족\n",
    "    - 발전 : 사전학습(Word2vec) , 문맥 임베딩(BERT , GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb284825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스를 만드는 애 : {'UNK': 1, 'my': 2, 'i': 3, 'love': 4, 'this': 5, 'really': 6, 'movie': 7, 'hate': 8, 'boring': 9, 'film': 10, 'suhan': 11, 'whole': 12, 'world': 13, 'in': 14, 'these': 15, 'frames': 16, 'bae': 17}\n",
      "원본 시퀀스 : [[3, 6, 4, 5, 7], [3, 8, 5, 9, 10], [2, 4, 11], [2, 12, 13, 14, 15, 16], [2, 17]]\n",
      "패딩 결과는? [[ 3  6  4  5  7  0  0  0]\n",
      " [ 3  8  5  9 10  0  0  0]\n",
      " [ 2  4 11  0  0  0  0  0]\n",
      " [ 2 12 13 14 15 16  0  0]\n",
      " [ 2 17  0  0  0  0  0  0]]  사이즈는? (5, 8)\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# sample data 만들기\n",
    "texts = [\n",
    "    ' I really love this movie ',\n",
    "    ' I hate this boring film ',\n",
    "    ' My love suhan ',\n",
    "    ' My whole world in these frames ',\n",
    "    ' My bae '\n",
    "]\n",
    "\n",
    "# 토큰화 객체 (최대 단어 10 , oov 토큰 지정)\n",
    "tokenizer = Tokenizer(num_words=20, oov_token='UNK') # OOV란 “사전에 없는 단어”를 뜻해서 시퀀스에는 사전에 없는 단어를 1로 표시\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(f'단어 인덱스를 만드는 애 : {tokenizer.word_index}')\n",
    "\n",
    "# 시퀀스를 맞춰줘야함 ( 문장을 숫자로 바꾸는 것 )\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "print(f'원본 시퀀스 : {seqs}')\n",
    "\n",
    "# 길이를 맞춰줘야하기때문에 padding을 사용.\n",
    "# 패딩 (최대 길이 8개)\n",
    "padded = pad_sequences(seqs , maxlen=8 , padding='post')\n",
    "print(f'패딩 결과는? {padded}  사이즈는? {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "임베딩 tensor 모양 : (5, 8, 4)\n",
      "첫 문장의 첫 단어 벡터는 ?  [ 0.04378352 -0.01106647  0.04632774  0.02125299]\n"
     ]
    }
   ],
   "source": [
    "# 단어 사이의 연관성이 없어.\n",
    "# 그래서 임베딩을 하는거임.\n",
    "# 임베딩 레이어를 사용할꺼임\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 패딩된 시퀀스인 padded를 사용할꺼임\n",
    "vocab_size = 18 # UNK를 포함한 단어 인덱스 최대값에 + 1\n",
    "embed_dim = 4 # 작은 차원 수\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size , output_dim=embed_dim , input_length=8)\n",
    "])\n",
    "embeddings = model.predict(padded)\n",
    "print(f'임베딩 tensor 모양 : {embeddings.shape}') #(5 , 8 , 4) 가 나올꺼같음\n",
    "print(f'첫 문장의 첫 단어 벡터는 ?  {embeddings[0,0,:]}') # I 에 대한 벡터값이 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5504536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 shape  (1, 6, 4)\n",
      "출력 shape  (1, 4, 2)\n",
      "출력값  [[[1.1425129  0.83391035]\n",
      "  [0.         0.        ]\n",
      "  [1.0220065  0.09407622]\n",
      "  [0.38864788 0.31862098]]]\n"
     ]
    }
   ],
   "source": [
    "# 1D Convoltion을 봐보자\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 임의의 시퀀스를 지정 (배치 1 , 길이 6 , 임베딩 4)\n",
    "x = np.random.rand(1,6,4).astype('float32')\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 2,  # 2개의 패턴을 감지할꺼야. 부정? 긍정?처럼 감지하는 단계\n",
    "    kernel_size = 3,  # 3-gram 3x3 네모로 찍는거임.\n",
    "    activation = 'relu' # 패턴을 감지하고 활성함수 통과 시킴\n",
    ")\n",
    "\n",
    "y = conv(x)\n",
    "print(f'입력 shape  {x.shape}')\n",
    "print(f'출력 shape  {y.shape}') # 입력값의 6개를 3x3네모로 옮겨다니니까 4 출력\n",
    "print(f'출력값  {y.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2489de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pooling : (1, 4, 2)\n",
      "after pooling : (1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# MaxPooling\n",
    "\n",
    "pooling = tf.keras.layers.MaxPool1D(pool_size=2) # 절반으로 줄어들게함\n",
    "pooled = pooling(y)\n",
    "print(f'before pooling : {y.shape}')\n",
    "print(f'after pooling : {pooled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f44da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에 있는 소스를 다 합쳐보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe083c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # 단어사전 만드는 모듈 / 단어를 숫자로 변경\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 길이 맞추는 모듈\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# sample data 만들기\n",
    "texts = [\n",
    "    ' I hate this boring film ',\n",
    "    ' My love suhan ',\n",
    "    ' I hate sick ',\n",
    "    ' My whole world in these frames '\n",
    "    \n",
    "]\n",
    "\n",
    "label = np.array([1,0,1,0])\n",
    "\n",
    "# LLM 가면 많은 tokenizer 가 많아짐. LLM은 순서가 중요하니 순서 기억해\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30 , oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts) # 단어 사전을 생성함.\n",
    "\n",
    "seqs = tokenizer.fit_on_sequences(texts) # 단어 사전 기반으로 단어들을 숫자로 변경 ( 길이는 다름! )\n",
    "\n",
    "# 길이를 맞춰야하는데 길이 맞춘 변수가 학습 데이터가 됨.\n",
    "padded = pad_sequences(seqs , maxlen=6 , padding='post')\n",
    "# 여기서 padded는 학습할 건덕지가 없음. 숫자로 되어 있는 데이터일 뿐 , 학습할 데이터로 만들어줘야함.\n",
    "# 벡터화 해서 비슷한 방향을 바라볼 수 있도록 만들어줘야해. 그걸 해주는게 embedding임.\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(51,8,input_length=6),\n",
    "    tf.keras.layers.Conv1D(16,3,activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "])\n",
    "# tensorflow는 알아서 값을 넣어 계산해주는데 파이토치는 레이어별로 다 해줘야함. 계산도 다 해야함.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토큰화 객체 (최대 단어 10 , oov 토큰 지정)\n",
    "tokenizer = Tokenizer(num_words=20, oov_token='UNK') # OOV란 “사전에 없는 단어”를 뜻해서 시퀀스에는 사전에 없는 단어를 1로 표시\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(f'단어 인덱스를 만드는 애 : {tokenizer.word_index}')\n",
    "\n",
    "# 시퀀스를 맞춰줘야함 ( 문장을 숫자로 바꾸는 것 )\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "print(f'원본 시퀀스 : {seqs}')\n",
    "\n",
    "# 길이를 맞춰줘야하기때문에 padding을 사용.\n",
    "# 패딩 (최대 길이 8개)\n",
    "padded = pad_sequences(seqs , maxlen=8 , padding='post')\n",
    "print(f'패딩 결과는? {padded}  사이즈는? {padded.shape}')\n",
    "\n",
    "# 단어 사이의 연관성이 없어.\n",
    "# 그래서 임베딩을 하는거임.\n",
    "# 임베딩 레이어를 사용할꺼임\n",
    "\n",
    "\n",
    "# 패딩된 시퀀스인 padded를 사용할꺼임\n",
    "vocab_size = 18 # UNK를 포함한 단어 인덱스 최대값에 + 1\n",
    "embed_dim = 4 # 작은 차원 수\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size , output_dim=embed_dim , input_length=8)\n",
    "])\n",
    "embeddings = model.predict(padded)\n",
    "print(f'임베딩 tensor 모양 : {embeddings.shape}') #(5 , 8 , 4) 가 나올꺼같음\n",
    "print(f'첫 문장의 첫 단어 벡터는 ?  {embeddings[0,0,:]}') # I 에 대한 벡터값이 출력됨\n",
    "\n",
    "\n",
    "# 1D Convoltion을 봐보자\n",
    "# 임의의 시퀀스를 지정 (배치 1 , 길이 6 , 임베딩 4)\n",
    "x = np.random.rand(1,6,4).astype('float32')\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 2,  # 2개의 패턴을 감지할꺼야. 부정? 긍정?처럼 감지하는 단계\n",
    "    kernel_size = 3,  # 3-gram 3x3 네모로 찍는거임.\n",
    "    activation = 'relu' # 패턴을 감지하고 활성함수 통과 시킴\n",
    ")\n",
    "\n",
    "y = conv(x)\n",
    "print(f'입력 shape  {x.shape}')\n",
    "print(f'출력 shape  {y.shape}') # 입력값의 6개를 3x3네모로 옮겨다니니까 4 출력\n",
    "print(f'출력값  {y.numpy()}')\n",
    "\n",
    "\n",
    "# MaxPooling\n",
    "pooling = tf.keras.layers.MaxPool1D(pool_size=2) # 절반으로 줄어들게함\n",
    "pooled = pooling(y)\n",
    "print(f'before pooling : {y.shape}')\n",
    "print(f'after pooling : {pooled.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
